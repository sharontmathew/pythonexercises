# -*- coding: utf-8 -*-
"""Twitter S A.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fyssKUIoyRMvKI29HKgPePKAYUBSm02c

Sentimental Analysis on Twittter Data

Sentiment analysis is a method analyse the sentiment behind a given piece of text.
Sentiment analysis is helps inÂ social media monitoring, and to gain an overview of the wider public opinion behind certain topics.

In this hateful racist and sexist sentiments are classified from the tweets

```
Importig the dataset
```
"""

import pandas as pd
import numpy as np

#importing the dataset using the function pd.read_csv
data = pd.read_csv("/content/drive/MyDrive/DataSets/twitter-sentiments.csv")
data.head(10)

# Commented out IPython magic to ensure Python compatibility.
#importinf the required libraries

import matplotlib.pyplot as plt
import seaborn as sns
import re
import string
import nltk
import warnings
# %matplotlib inline

warnings.filterwarnings('ignore')

# datatype info
data.info()

pd.get_dummies(data['label'])

#balance the dataset
data['label'].value_counts()

"""Preprocessing """

# removing pattern 

def remove_pattern(input_txt, pattern):
    remove = re.findall(pattern, input_txt)
    for word in remove:
        input_txt = re.sub(word, "", input_txt)
    return input_txt

# remove twitter handles (@user)
data['clean_tweet'] = np.vectorize(remove_pattern)(data['tweet'], "@[\w]*")
data.head()

# remove special characters, numbers and punctuations
data['clean_tweet'] = data['clean_tweet'].str.replace("[^a-zA-Z#]", " ")
data.head()

#downloading stopwords
nltk.download('stopwords')

nltk.download('punkt')

#importing word tokkeniser and Stemmer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem.porter import PorterStemmer
stemmer = PorterStemmer()

stop_words = [stemmer.stem(word) for word in clean_tweet
                if not word in set(stopwords.words('english'))]

#  Tokenization

stop_words = data['clean_tweet'].apply(lambda word: word.split())
stop_words.head()

# Stemmed words to sentence
for i in range(len(stop_words)):
    stop_words[i] = " ".join(stop_words[i])
    
data['clean_tweet'] = stop_words
data.head()

# remove short words
data['clean_tweet'] = data['clean_tweet'].apply(lambda x: " ".join([w for w in x.split() if len(w)>3]))
data.head()

"""Exploratory Data Analysis"""

#Word Cloud

#Joining all the sentences to a single sentence
all_words = " ".join([sentence for sentence in data['clean_tweet']])

from wordcloud import WordCloud
wordcloud = WordCloud(width=800, height=500, colormap = 'RdYlGn', max_words = 100,  
random_state=42, max_font_size=100).generate(all_words)

# plot the graph
plt.figure(figsize=(15,8))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

#for all the positive sentences 

all_words = " ".join([sentence for sentence in data['clean_tweet'][data['label']==0]])

from wordcloud import WordCloud
wordcloud = WordCloud(width=800, height=500, colormap = 'RdYlGn', max_words = 50,  
random_state=42, max_font_size=100).generate(all_words)

# plot the graph
plt.figure(figsize=(15,8))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

all_words = " ".join([sentence for sentence in data['clean_tweet'][data['label']==1]])

from wordcloud import WordCloud
wordcloud = WordCloud(width=800, height=500, colormap = 'RdYlGn', max_words = 50,  
random_state=42, max_font_size=100).generate(all_words)

# plot the graph
plt.figure(figsize=(15,8))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

# hashtags

def hashtag_extract(tweets):
    hashtags = []
    # loop words in the tweet
    for tweet in tweets:
        htags = re.findall(r"#(\w+)", tweet)
        hashtags.append(htags)
    return hashtags

# extract hashtags from positive tweets
htags_positive = hashtag_extract(data['clean_tweet'][data['label']==0])
htags_positive[:10]

# extract hashtags from megative tweets
htags_negative = hashtag_extract(data['clean_tweet'][data['label']==1])
htags_negative[:10]

#Converting the hashtags to a single string
htag_positive = sum(htags_positive, []) #positive string
htag_negative = sum(htags_negative, []) #Negative string

#coverting the hashtags and its count to a dataframe
freq = nltk.FreqDist(htag_positive)
df = pd.DataFrame({'Hashtag': list(freq.keys()),
                 'Count': list(freq.values())})
df.head()

#  10 top hashtags
df = df.nlargest(columns='Count', n=10)
plt.figure(figsize=(15,9))
sns.barplot(data=df, x='Hashtag', y='Count')
plt.show()

"""Spliting the dataset"""

# feature extraction
from sklearn.feature_extraction.text import CountVectorizer
bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')
bow = bow_vectorizer.fit_transform(data['clean_tweet'])

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(bow, data['label'], random_state=42, test_size=0.25)

"""Model Training"""

#Logistic Regression
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

model = LogisticRegression()
model.fit(x_train, y_train)
pred = model.predict(x_test)

accuracy_score(y_test,pred)

#Naive Bayes
from sklearn.naive_bayes import MultinomialNB

model = MultinomialNB()
model.fit(x_train, y_train)
pred = model.predict(x_test)

accuracy_score(y_test,pred)

#Decision Tree
from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier()
model.fit(x_train, y_train)
pred = model.predict(x_test)

accuracy_score(y_test,pred)









